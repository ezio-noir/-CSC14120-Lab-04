{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGBgThS8q8k3"
      },
      "source": [
        "Full name: Tô Hiển Vinh\n",
        "\n",
        "Student ID: 21120365"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qdrvDrCrnqz"
      },
      "source": [
        "# HW4: Parallel Radix Sort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKXB0wA7yhq9"
      },
      "source": [
        "**To compile your file, you can use this command:** \\\n",
        "`nvcc tên-file.cu -o tên-file-chạy` \\\n",
        "***You can use Vietnamese to anwser the questions***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf '-CSC14120-Lab-04'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ibRxIq5MJO_",
        "outputId": "dd986599-d1f9-4826-866d-d8c7aa4e3aff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: invalid option -- 'C'\n",
            "Try 'rm ./-CSC14120-Lab-04' to remove the file '-CSC14120-Lab-04'.\n",
            "Try 'rm --help' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tbFLx1i4JxIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8542302e-1de3-4a7b-e5ef-582a0a690766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing source code. Attempt to clone from Github...\n",
            "Cloning into 'src'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 5 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (5/5), done.\n"
          ]
        }
      ],
      "source": [
        "!if ! [ -f \"HW4.cu\" ]; then \\\n",
        "    echo \"Missing source code. Attempt to clone from Github...\"; \\\n",
        "    git clone \"https://github.com/ezio-noir/-CSC14120-Lab-04.git\" \"src\"; \\\n",
        "    cp \"src/HW4.cu\" .; \\\n",
        "else \\\n",
        "    echo \"Found source code.\"; \\\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aZNqZuECjNso"
      },
      "outputs": [],
      "source": [
        "!nvcc HW4.cu -o HW4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NVFUj14OYUyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0daf99dd-b44b-4aae-9a62-009791449735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9497.573 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Time: 1025.334 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "# Run with block size 256\n",
        "!./HW4 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run with block size 512\n",
        "!./HW4 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCBcX-GLMqnn",
        "outputId": "66043585-43d2-481d-de0c-4e6611e78d25"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 10125.523 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Time: 579.654 ms\n",
            "CORRECT :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run with block size 1024\n",
        "!./HW4 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ufk_JbRMuli",
        "outputId": "a4511a3b-ad83-4320-cdb8-6f90ca5246da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835660288 byte\n",
            "SMEM per SM: 65536 byte\n",
            "SMEM per block: 49152 byte\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Radix Sort by host\n",
            "Time: 9409.847 ms\n",
            "\n",
            "Radix Sort by device\n",
            "Time: 421.923 ms\n",
            "CORRECT :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Device execution time seems to be reduced as the block size increases. Specifically in my runtime on Colab's Tesla T4, the execution time is as follows:  \n",
        "\n",
        "|Version | Time (ms)|   \n",
        "|------- | ---------|\n",
        "| Host (run with block size = 256 version) | 9497.573 |\n",
        "| Host (run with block size = 512 version) | 10125.523 |\n",
        "| Host (run with block size = 1024 version) | 9409.847 |\n",
        "| Device (block size = 256) | 1025.334 |\n",
        "| Device (block size = 512) | 579.654 |\n",
        "| Device (block size = 1024) | 421.923 |\n",
        "\n",
        "- Kernels execution time (link to `nsys` reports: [Google Drive](https://drive.google.com/drive/folders/1zviQ9bWn3vwCjZ4rxAnvpCT2BB0BlkS7?usp=sharing)):\n",
        "\n",
        "| Kernel | Average time (ms)|   \n",
        "|------- | ---------|\n",
        "| `g_extractBits` (block size = 256) | 0.520 - 0.559 |\n",
        "| `g_extractBits` (block size = 512) | 0.524 - 0.589 |\n",
        "| `g_extractBits` (block size = 1024) | 0.629 - 0.671 |\n",
        "| `g_scan` (block size = 256) | 16.4 - 21.4 |\n",
        "| `g_scan` (block size = 512) | 8.7 - 11.2 |\n",
        "| `g_scan` (block size = 1024) | 4.9 - 5.3 |\n",
        "| `g_computeRankAndAssign` (block size = 256) | 0.990 - 1.002 |\n",
        "| `g_computeRankAndAssign` (block size = 512) | 0.991 - 1.002 |\n",
        "| `g_computeRankAndAssign` (block size = 1024) | 0.995 - 0.998 |\n",
        "\n",
        "- We can see that `g_scan` takes most of the device execution time among the kernels, and mainly contributes the performances differences between the versions. We can also notice that `g_scan`'s execution time and block size are inversely proportional (i.e. execution time halves when the block size doubles).\n",
        "\n",
        "- Let $B = $ block size, $n = $ number of input elements (for simplicity, assume $n$ is an exponent of $2$), then $n_{B} = $ number of blocks $ = ⌈\\frac{n}{2B}⌉$, since each block handles a section of $2B$ elements.\n",
        "    - For each block $i$, the execution is made up of these steps:  \n",
        "        1. Get block ID and load data from global memory into shared memory.\n",
        "        2. Reduction phase.\n",
        "        3. Post-reduction phase.\n",
        "        4. Write block result to `block_sums` and wait for previous blocks to complete.\n",
        "        5. Write final result to `out`.\n",
        "    - Each block can execute step 1. and 5. concurrently and with same time (assumming getting block ID is neglible), since each has to perform loading $2B$ elements. Let $T_{1_i} \\approx T_1$ is time cost of this step.\n",
        "    - The reduction phase performs tree reduction on $2B$ elements, which make time complexity be $T_{2_i} \\approx T_2 = \\log_2{2B}$.\n",
        "    - The post-reduction phase time complexity is $T_{3_i} \\approx T_3 =\n",
        "    \\log_2{2B} - 1 = \\log_2{B}$ (1 step less than reduction phase).\n",
        "    - We can see that step 1, 2, and 3 are executed independenly by blocks, and cost roughly the same time.\n",
        "    - In step 4, a block $i$ must wait for all block $j, j < i$  to complete their steps 4 (only then the `complete_block_count` increases to be equal to `block_id`).\n",
        "    - So the total execution time for block $i$ is: $T_i = T_{1_i} + T_{2_i} + T_{3_i} + T_{4_i} + T_{5_i} ≈ T_1 + T_2 + T_3 + T_{4_i} + T_5 = T_1 + T_2 + T_3 + ∑_{j = 0}^{j = i - 1} T_4j + T_5 $.\n",
        "    - Then the execution time of the last block is the longest: $T_1 + T_2 + T_3 + ∑_{j = 0}^{j = n_b - 1} T_4j + T_5$.\n",
        "    - Since the kernel time is equal to the longest execution time of its blocks, we can see that the blocking of step 4 contributes to the difference in execution time when block size varies. For example, when the block size $B$ is doubled, then $n_b$ is halved, which make the last block wait less blocks before it to complete."
      ],
      "metadata": {
        "id": "60ip3rohM4iF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XebMjR45-0Io"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}